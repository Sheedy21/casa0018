{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kl-2YCEojEY"
      },
      "source": [
        "import numpy as np # For arithmetics and arrays\n",
        "import math # For inbuilt math functions\n",
        "import pandas as pd # For handling data frames\n",
        "import collections # used for dictionaries and counters\n",
        "from itertools import permutations # used to find permutations\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function to easily split data into training and testing samples\n",
        "from sklearn.decomposition import PCA # Principal component analysis used to reduce the number of features in a model\n",
        "from sklearn.preprocessing import StandardScaler # used to scale data to be used in the model\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import pickle # To save the trained model and then read it\n",
        "\n",
        "import seaborn as sns # Create plots\n",
        "sns.set(style=\"ticks\")\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QRhP2C8olBj"
      },
      "source": [
        "df = pd.read_csv('lang_data.csv') # Read raw data\n",
        "df = df.dropna() # remove null values for the \"text\" column\n",
        "df['text'] = df['text'].astype(str) # Convert the column \"text\" from object to a string in order to operate on it\n",
        "df['language'] = df['language'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-trX9UeMosfT"
      },
      "source": [
        "# Define a list of commonly found punctuations\n",
        "punc = ('!', \",\" ,\"\\'\" ,\";\" ,\"\\\"\", \".\", \"-\" ,\"?\")\n",
        "vowels=['a','e','i','o','u']\n",
        "# Define a list of double consecutive vowels which are typically found in Dutch and Afrikaans languages\n",
        "same_consecutive_vowels = ['aa','ee', 'ii', 'oo', 'uu'] \n",
        "consecutive_vowels = [''.join(p) for p in permutations(vowels,2)]\n",
        "dutch_combos = ['ij']\n",
        "\n",
        "# Create a pre-defined set of features based on the \"text\" column in order to allow us to characterize the string\n",
        "df['word_count'] = df['text'].apply(lambda x : len(x.split()))\n",
        "df['character_count'] = df['text'].apply(lambda x : len(x.replace(\" \",\"\")))\n",
        "df['word_density'] = df['word_count'] / (df['character_count'] + 1)\n",
        "df['punc_count'] = df['text'].apply(lambda x : len([a for a in x if a in punc]))\n",
        "df['v_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'v']))\n",
        "df['w_char_count'] = df['text'].apply(lambda x : len([a for a in x if a.casefold() == 'w']))\n",
        "df['ij_char_count'] = df['text'].apply(lambda x : sum([any(d_c in a for d_c in dutch_combos) for a in x.split()]))\n",
        "df['num_double_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in same_consecutive_vowels) for a in x.split()]))\n",
        "df['num_consec_vowels'] = df['text'].apply(lambda x : sum([any(c_v in a for c_v in consecutive_vowels) for a in x.split()]))\n",
        "df['num_vowels'] = df['text'].apply(lambda x : sum([any(v in a for v in vowels) for a in x.split()]))\n",
        "df['vowel_density'] = df['num_vowels']/df['word_count']\n",
        "df['capitals'] = df['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
        "df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['character_count']),axis=1)\n",
        "df['num_exclamation_marks'] =df['text'].apply(lambda x: x.count('!'))\n",
        "df['num_question_marks'] = df['text'].apply(lambda x: x.count('?'))\n",
        "df['num_punctuation'] = df['text'].apply(lambda x: sum(x.count(w) for w in punc))\n",
        "df['num_unique_words'] = df['text'].apply(lambda x: len(set(w for w in x.split())))\n",
        "df['num_repeated_words'] = df['text'].apply(lambda x: len([w for w in collections.Counter(x.split()).values() if w > 1]))\n",
        "df['words_vs_unique'] = df['num_unique_words'] / df['word_count']\n",
        "df['encode_ascii'] = np.nan\n",
        "for i in range(len(df)):\n",
        "    try:\n",
        "        df['text'].iloc[i].encode(encoding='utf-8').decode('ascii')\n",
        "    except UnicodeDecodeError:\n",
        "        df['encode_ascii'].iloc[i] = 0\n",
        "    else:\n",
        "        df['encode_ascii'].iloc[i] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlw_HW8rpBVT"
      },
      "source": [
        "#split dataset into features and target variable\n",
        "feature_cols = list(df.columns)[2:]\n",
        "X = df[feature_cols] # Features\n",
        "y = df[['language']] # Target variable\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% train and 20% test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDv_E7JRpCKw"
      },
      "source": [
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "# Fit on training set only.\n",
        "scaler.fit(X_train)\n",
        "# Transform both the training set and the test set.\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Make an instance of the model to retain 95% of the variance within the old features.\n",
        "pca = PCA(.95)\n",
        "pca.fit(X_train)\n",
        "\n",
        "print('Number of Principal Components = '+str(pca.n_components_))\n",
        "# Number of Principal Components = 13\n",
        "\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb83_KX_pSOl"
      },
      "source": [
        "dt_clf = DecisionTreeClassifier() # Create Decision Tree classifer object\n",
        "dt_clf = dt_clf.fit(X_train,y_train) # Fit/Train Decision Tree Classifer on training set\n",
        "\n",
        "# Save model to file in the current working directory so that it can be imported and used.\n",
        "# I use the pickle library to save the parameters of the trained model\n",
        "pkl_file = \"decision_tree_model.pkl\"\n",
        "with open(pkl_file, 'wb') as file:\n",
        "    pickle.dump(dt_clf, file)\n",
        "\n",
        "# Load previously trained model from pickle file\n",
        "with open(pkl_file, 'rb') as file:\n",
        "    dt_clf = pickle.load(file)\n",
        "\n",
        "dt_clf # parameters of the Decision Tree model are shown below and can be further optimized to improve model performance\n",
        "\n",
        "y_pred = dt_clf.predict(X_test) #Predict the response for test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2azpe3vLpX-V"
      },
      "source": [
        "labels = [‘English’, ‘Afrikaans’, ‘Nederlands’]\n",
        "# Confusion Matrix\n",
        "cm_Model_dt = confusion_matrix(y_test, y_pred, labels)\n",
        "fig = plt.figure(figsize=(9,9))\n",
        "ax = fig.add_subplot(111)\n",
        "sns.heatmap(cm_Model_dt, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "ax.set_xticklabels(labels)\n",
        "ax.set_yticklabels(labels)\n",
        "title = 'Decision Tree Model Accuracy Score = '+ str(round(accuracy_score_dt*100,2)) +\"%\"\n",
        "plt.title(title, size = 15)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}